{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Python Tutorial 3 - 3.ipynb","provenance":[],"authorship_tag":"ABX9TyOOTN3c3wgbbmO7031K7f9X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OQtPkwn1kKnf"},"source":["# Autograd implementation\n","\n","In a neural network, we pass the input through a network layer and calculate the output of the layer straightforwardly. This step is called forward-propagation. Each layer also implements a function called 'backward'. Backward is responsible for the backward pass of back-propagation. The process of back-propagation follows the schemas: Input -> Forward calls -> Loss function -> derivative -> back-propagation of errors. In a neural network, any layer can forward its results to many other layers, in this case, in order to do back-propagation, we sum the deltas coming from all the target layers. "]},{"cell_type":"code","metadata":{"id":"Ko0kWtlUjutN"},"source":["'''backprop implementation with layer abstraction.'''\n","import numpy as np\n","\n","class Layer:\n","    '''A layer in a network.\n","\n","    A layer is simply a function from R^n to R^d for some specified n and d.\n","    A neural network can usually be written as a sequence of layers:\n","    if the original input x is in R^d, a 3 layer neural network might be:\n","\n","    L3(L2(L1(x)))\n","\n","    We can also view the loss function as itself a layer, so that the loss\n","    of the network is:\n","\n","    Loss(L3(L2(L1(x))))\n","\n","    This class is a base class used to represent different kinds of layer\n","    functions. We will eventually specify a neural network and its loss function\n","    with a list:\n","\n","    [L1, L2, L3, Loss]\n","\n","    where L1, L2, L3, Loss are all Layer objects.\n","\n","    Each Layer object implements a function called 'forward'. forward simply\n","    computes the output of a layer given its input. So instead of\n","    Loss(L3(L2(L1(x))), we write\n","    Loss.forward(L3.forward(L2.forward(L1.forward(x)))).\n","    Doing this computation finishes the forward pass of backprop.\n","\n","    Each layer also implements a function called 'backward'. Backward is\n","    responsible for the backward pass of backprop. After we have computed the\n","    forward pass, we compute\n","    L1.backward(L2.backward(L3.backward(Loss.backward(1))))\n","    We give 1 as the input to Loss.backward because backward is implementing\n","    the chain rule - it multiplies gradients together and so giving 1 as an\n","    input makes the multiplication an identity operation.\n","\n","    The outputs of backward are a little subtle. Some layers may have a\n","    parameter that specifies the function being computed by the layer. For\n","    example, a Linear layer maintains a weight matrix, so that\n","    Linear(x) = xW for some matrix W.\n","    \n","    The input to backward should be the gradient of the final loss with respect\n","    to the output of the current layer. The output of backward should be the\n","    gradient of the final loss with respect to the input of the current layer,\n","    which is just the output of the previous layer. This is why it is correct\n","    to chain the outputs of backprop together. However, backward should ALSO\n","    compute the gradient of the loss with respect to the current layer's\n","    parameter and store this internally to be used in training.\n","    '''\n","    def __init__(self, parameter=None, name=None):\n","        self.name = name\n","        self.forward_called = False\n","        self.parameter = parameter\n","        self.grad = None\n","\n","    def zero_grad(self):\n","        self.grad = None\n","\n","    def forward(self, input):\n","        '''forward pass. Should compute layer and save relevant state\n","        needed for backward pass.\n","        Args:\n","            input: input to this layer.\n","        returns output of operation.\n","        '''\n","        raise NotImplementedError\n","\n","    def backward(self, downstream_grad):\n","        '''Performs backward pass.\n","\n","        This function should also set self.grad to be the gradient of the final\n","        output of the computation with respect to the parameter.\n","\n","        Args:\n","            downstream_grad: gradient from downstream operation in the\n","                computation graph. This package will only consider\n","                computation graphs that result in scalar outputs at the final\n","                node (e.g. loss function computations). As a result,\n","                the dimension of downstream_grad should match the dimension of\n","                the output of this layer.\n","\n","                Formally, if this operation computes F(x), and the final\n","                computation computes a scalar, G(F(x)), then input_grad is\n","                dG/dF.\n","        returns:\n","            gradient to pass to upstream layers. If the layer computes F(x, w),\n","            where x is the input and w is the parameter of the layer, then\n","            the return value should be dF(x,w)/dx * downstream_grad. Here,\n","            x is in R^n, F(x, w) is in R^m, dF(x, w)/dx is a matrix in R^(n x m)\n","            downstream_grad is in R^m and * indicates matrix multiplication.\n","\n","        We should also compute the gradient with respect to the parameter w.\n","        Again by chain rule, this is dF(x, w)/dw * downstream_grad\n","        '''\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKZ3SP2lTpSJ"},"source":["Below shows an example of the full implementation of a bias layer, including the forward and backward function. Notice self.grad stores the gradient of the loss with respect to the current layer's parameter."]},{"cell_type":"code","metadata":{"id":"JF30rs0ITosh"},"source":["class Bias(Layer):\n","    '''adds a constant bias.\n","    The bias is assumed to be a K-dimensional ndarray for arbitary K that is\n","    added to the last K dimensions of a D dimensional input and broadcast\n","    over the first D-K dimensions.'''\n","\n","    def __init__(self, bias, name=\"bias\"):\n","        super(Bias, self).__init__(np.squeeze(bias), name)\n","        self.weights = np.squeeze(bias)\n","\n","    def forward(self, input):\n","        self.input = input\n","        return self.parameter + self.input\n","\n","    def backward(self, downstream_grad):\n","        self.grad = np.sum(downstream_grad, tuple(range(downstream_grad.ndim - self.parameter.ndim)))\n","        return downstream_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntLOrh72mHOW"},"source":["## Multiplication layer.\n","\n","Let's look at the basic linear layer.\n","\n","$Z_{linear} = XW$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UhpqN_9O2fmz"},"source":["Assuming that loss $L$ is a function of $Z$, What is $\\frac{\\partial{L}}{\\partial{X}}$ ?\n","\n","$Z$, $X$ and $W$ are each $2$-tensors, therefore the derivative of any one of them w.r.t. any other will be a $4$-tensor. $Z$ has size $(B \\times M)$, $X$ has size $(B \\times N)$, and $W$ has size $(N \\times M)$.\n","\n","Focusing on the $\\{i,j\\}$ element of $Z$:\n","\n","$$\n","Z_{ij} = \\sum_{k=1}^B X_{ik} W_{kj}\n","$$\n","\n","$\\partial{Z_{ij}}/\\partial{X}$ is a $(B \\times N)$ matrix, but is only nonzero in row $i$, where it's values are the $j$th column of $W$. Therefore the full derivative of $Z$ w.r.t. $X$ is a sparse Jacobian defined by:\n","\n","$$\n","\\boxed{\\left(\\frac{\\partial{Z}}{\\partial{X}}\\right)_{ijik} = W_{kj}}\n","$$\n","\n","with all other values equal to zero.\n","\n","When applying the chain rule for backpropagation from a downstream function $L(Z)$, we do not need to compute the full jacobian $\\partial{Z}/\\partial{X}$. Instead, we get:\n","\n","$$\n","\\frac{\\partial{L}}{\\partial{X}} = \\frac{\\partial{L}}{\\partial{Z}}\\frac{\\partial{Z}}{\\partial{X}} = \\frac{\\partial{L}}{\\partial{Z}} W^T\n","$$\n","\n","This is merely a convenient rearrangement of the terms of $\\partial{Z}/\\partial{X}$."]},{"cell_type":"markdown","metadata":{"id":"5gyrJM3O2k2U"},"source":["Using this, we can complete the forward and backward function of the linear layer. In backward, we ALSO set the self.grad to be the gradient of the loss with respect to the current layer's parameter."]},{"cell_type":"code","metadata":{"id":"4L8Dcfq-lTRd"},"source":["class Linear(Layer):\n","    '''Linear layer. Parameter is NxM matrix L, input is matrix v of size B x N\n","    where B is batch size, output is vL.'''\n","\n","    def __init__(self, weights, name=\"Linear\"):\n","        super(Linear, self).__init__(weights, name)\n","        self.parameter = weights\n","\n","    def forward(self, input):\n","        self.input = input\n","        return self.input @ self.parameter\n","\n","    def backward(self, downstream_grad):\n","        '''downstream_grad should be BxM.'''\n","        self.grad = np.transpose(self.input) @ downstream_grad #dL/dW = X' * dL/dZ\n","        return downstream_grad @ np.transpose(self.parameter) #dL/dX = dL/dZ * W'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"icklJkILnhNB"},"source":["## Activation layers.\n","\n","Now let's look at a popular activation layer:\n","\n","$ReLU(x) = max(0,x)$\n"]},{"cell_type":"markdown","metadata":{"id":"nq5dtn6k49SR"},"source":["$ReLU(x)$ Derivative:\n","\n","$$\n","ReLU(x)\n","= \\left\\{ \\begin{array}{rcl}\n","0 & \\mbox{for} & x \\le 0 \\\\\n","x & \\mbox{for} & x > 0\n","\\end{array}\\right.\n","$$\n","\n","$$\n","\\boxed{\\frac{\\partial}{\\partial{x}}ReLU(x)\n","= \\left\\{ \\begin{array}{rcl}\n","0 & \\mbox{for} & x \\le 0 \\\\\n","1 & \\mbox{for} & x > 0\n","\\end{array}\\right.}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"NZPP7OTt49NQ"},"source":["Now we can write the forward and backward functions for this layer. There is no need to update self.grad since there is no parameter in activation layers. "]},{"cell_type":"code","metadata":{"id":"FgbalOrBlYkS"},"source":["class ReLU(Layer):\n","    '''ReLU layer. No parameters.'''\n","\n","    def __init__(self, name=\"ReLU\"):\n","        super(ReLU, self).__init__(name=name)\n","\n","    def forward(self, input):\n","        self.input = input\n","        return np.maximum(0,self.input)\n","\n","    def backward(self, downstream_grad):\n","        upstream_grad = downstream_grad.copy()\n","        upstream_grad[self.input <= 0] = 0;\n","        return upstream_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zU1LRCr_oI5o"},"source":["##Another Example: Softmax\n","\n","$\\sigma(j)=\\frac{\\exp(w_j^Tx)}{\\sum_{k}\\exp(w_k^Tx)}$."]},{"cell_type":"markdown","metadata":{"id":"VghiDCb45L4M"},"source":["When $i = j$,\n","\n","$$\n","\\frac{\\partial}{\\partial{x_i}}\\sigma(j) \\ = \\ w_i \\sigma(i)(1-\\sigma(i))\n","$$\n","\n","When $i \\ne j$,\n","$$\n","\\frac{\\partial}{\\partial{x_i}}\\sigma(j) \\ = -w_i\\sigma(i)\\sigma(j)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"EyxKVXh76VIi"},"source":["Now we can write the forward and backward functions for a softmax layer. There is no need to update self.grad since there is no parameter in a softmax function."]},{"cell_type":"code","metadata":{"id":"CxdqItKPlY-x"},"source":["class SoftMax(Layer):\n","    '''SoftMax Layer, no parameters.'''\n","\n","    def __init__(self, name=\"SoftMax\"):\n","        super(SoftMax, self).__init__(name=\"softmax\")\n","\n","    def forward(self, input):\n","        '''input is BxN array, B is batch dimension.'''\n","        e = np.exp(input - np.max(input)) #max over entire matrix\n","        self.p = e / np.sum(e, axis=1)[:,None]\n","        return self.p\n","\n","    def backward(self, downstream_grad):\n","        '''downstream grad should be BxN array.'''\n","        return self.p * (downstream_grad - np.sum(downstream_grad * self.p, axis=1)[:,None])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4oo-_OBxclg"},"source":["... And so on."]}]}